{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "k7KtT5ddSbhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the folder containing CSV files\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "# Combine all CSV files into one DataFrame\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Specify the semicolon as the delimiter\n",
        "        temp_data = pd.read_csv(file_path, sep=';')\n",
        "        all_data = pd.concat([all_data, temp_data], ignore_index=True)\n",
        "\n",
        "# Display the combined dataset\n",
        "print(\"Combined DataFrame:\")\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "w9QyjAF8ScX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load one of the CSV files for inspection\n",
        "file_path = '/content/drive/My Drive/Preprocessed/HUPA0015P.csv'  # Replace with the actual path\n",
        "data = pd.read_csv(file_path, sep=';', header=None)  # Use sep=';' to handle semicolon delimiter\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "9EfPu530Sf5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path, sep=';', header=0, engine='python')  # Use 'python' engine for better compatibility\n",
        "print(data.columns)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "0CLRjWaDSkti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read and split the data\n",
        "        temp_data = pd.read_csv(file_path, header=None)\n",
        "        temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "        temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "        # Combine into the main DataFrame\n",
        "        all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "r4f3ip8ZSlYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "# Handle missing values\n",
        "all_data = all_data.dropna()\n",
        "\n",
        "# Separate features and target\n",
        "target_column = 'glucose'  # Replace with your actual target column name\n",
        "X = all_data.drop(columns=[target_column, 'time'])  # Drop 'time' column from features\n",
        "y = all_data[target_column]\n",
        "\n",
        "# Convert columns to numeric, errors='coerce' to handle potential non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UUPCWd9USojq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVM_aN90SMwn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "# Handle missing values\n",
        "all_data = all_data.dropna()\n",
        "\n",
        "# Separate features and target\n",
        "target_column = 'glucose'  # Replace with your actual target column name\n",
        "X = all_data.drop(columns=[target_column, 'time'])  # Drop 'time' column from features\n",
        "y = all_data[target_column]\n",
        "\n",
        "# Convert columns to numeric, errors='coerce' to handle potential non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "y = pd.to_numeric(y, errors='coerce') # Convert y to numeric as well\n",
        "\n",
        "# Replace infinite values with NaN so they can be imputed\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Impute NaN values using the mean (or another strategy)\n",
        "X = X.fillna(X.mean())\n",
        "y = y.fillna(y.mean())\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "predictions = xgb_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"XGBoost Mean Squared Error:\", mse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# ... (Your existing XGBoost code: data loading, preprocessing, model training) ...\n",
        "\n",
        "# Evaluate\n",
        "predictions_xgb = xgb_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions_xgb)\n",
        "print(\"XGBoost Mean Squared Error:\", mse)\n",
        "\n",
        "# --- Categorization for Classification Metrics ---\n",
        "thresholds = [70, 140]  # Example thresholds (adjust as needed)\n",
        "y_test_cat = np.digitize(y_test, thresholds)\n",
        "predictions_xgb_cat = np.digitize(predictions_xgb, thresholds)\n",
        "\n",
        "# --- Classification Metrics ---\n",
        "cm_xgb = confusion_matrix(y_test_cat, predictions_xgb_cat)\n",
        "precision_xgb = precision_score(y_test_cat, predictions_xgb_cat, average='weighted')\n",
        "recall_xgb = recall_score(y_test_cat, predictions_xgb_cat, average='weighted')\n",
        "accuracy_xgb = accuracy_score(y_test_cat, predictions_xgb_cat)\n",
        "f1_xgb = f1_score(y_test_cat, predictions_xgb_cat, average='weighted')\n",
        "\n",
        "print(\"\\nXGBoost Classification Metrics:\")\n",
        "print(\"Confusion Matrix:\", cm_xgb)\n",
        "print(\"Precision:\", precision_xgb)\n",
        "print(\"Recall:\", recall_xgb)\n",
        "print(\"Accuracy:\", accuracy_xgb)\n",
        "print(\"F1-score:\", f1_xgb)"
      ],
      "metadata": {
        "id": "O837D1NVSWpr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}